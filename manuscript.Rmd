---
title: "Bayesian Statistics Assignment"
author: "Sof√≠a Villalobos Aliste (6060714)"
date: "10-01-2022"
output: pdf_document
---

```{r, echo=F, message=F, warning=F}
library(ggplot2)
library(tidyjson)
library(readr)
library(bain)
library(dplyr)
library(knitr)
options(scipen =999)
```

```{r, echo=F, message=F, warning=F}
df <- read.csv("df.csv")
df<-df[-c(1)]
N<-(length(df$scale))
Y<-df$scale
X1<-df$prl
X2<-df$lgl
X3<-df$plc
```

# Research question

The aim of this project is to examine if Political Self-Positioning in the Netherlands can be predicted from variables concerning Institutional Trust: Trust on country's parliament, Trust in legal system, and Trust in Police. This would be addressed modelling the following regression:$$Y=B_0+B_1*X1+B_2*X2+B_3*X3+\varepsilon$$

# Data

The data for this research had been obtained from the European Social Survey 9th round, collected in 2018 in the Netherlands.

The dependent variable that will be used as political self-positioning is "Placement on left right scale" and consists in 10 categories going from 0 (Left) to 10 (Right). Independent variables Trust in country's parliament (prl), Trust in the legal system (lgl), and Trust in the police (plc), has are categorical variables that goes from 0 (No trust at all) to 10 (Complete trust). All these variables will be here treated as continuous.

Two models of multiple linear regression are proposed here that differ in the type of previous information used: for the first model we included specified priors by taking in account the round 8 of the European Social Survey conducted in the Netherlands in 2016. The second model was constructed with the same variables but uninformative priors instead.

The total cases for this survey used in the analysis is 1524 after using listwise deletion.

# Estimation and convergence

The estimation process was made through Gibbs Sampling, for $B_0, B_2, B_3$, and $\sigma^{2}$, and with Metropolis Hasting sampling for the estimation of $B_1$. First, we define the initial values for each of the two chains that we will be using to sample, and the number of iterations (10000 for each).

$B_0$ conditional posterior was obtained by derivating first the initial values to obtain conditional posterior mean and variance, and then sampling a value from conditional posterior of $B_0$, and we keep this value for the next step of sampling $B_1$. $B_2$ and $B_3$ are sampled in the same way.

B1 was obtained through a Metropolis Hasting step, were we start taking the previously obtained value for $B_0$, to derive the conditional mean and variance, and with these values we sample from a normal proposal distribution a value for $B_1$, that could be accepted or rejected. according if it the value we sample from a uniform distribution is larger or smaller in comparison to the acceptance ratio that is calculated computing the target densities that can be computed from the conditional mean and variance. This is done as a correction in case of obtaining too improbable values. Then, if the proposal value of $B_1$ is rejected for being larger than the ratio, the last value of $B_1$ is retained until there is a proposal value of $B_1$ that is equal or smaller to this ratio.

Finally, we sample the posterior distribution for the residual variances. Using the sampled values of $B_0, B_1, B_2$ and $B_3$ to compute the condition posterior of $\sigma^{2}$ by sampling posterior shape parameter alfa and posterior scale parameter beta from where we can compute the sum of squared residuals.

That is how the sampler works and was used to estimate the parameters for Model 1 (with informative priors) and Model 2 (with uninformative priors).

```{r,echo=F, message=F, warning=F}
#################### DEFINE INFORMATIVE PRIORS ####################
#B0
mu00<-0
tau00<-1000
#B1
mu10<-5.477136
tau10<-3.991951
#B2
mu20<-6.201686
tau20<-4.085202
#B3
mu30<-6.665075
tau30<-2.936145
#S2
alfa0<-.01
beta0<-.01
# DEFINE NUMBER OF ITERATIONS AND STORAGE OF RESULTS
n.iterations <- 10000
results1 <- matrix(0,n.iterations,5)
results2 <- matrix(0,n.iterations,5)
colnames(results1)<-c("b0", "b1", "b2","b3", "var")
colnames(results2)<-c("b0", "b1", "b2","b3", "var")
# DEFINE INITIAL VALUES FOR ITERATIONS OF TWO CHAINS
b0=-3
b1=5
b2=-8
b3=.6
var=1
b0_2=3
b1_2=-5
b2_2=8
b3_2=-.6
var1=2
results1[1,] <- c(b0, b1, b2,b3, var)
results2[1,] <- c(b0_2, b1_2, b2_2,b3_2, var1)
#################### DEFINE UNINFORMATIVE PRIORS ####################
#B0
mu00<-0
tau00<-1000
#B1
mu10<-0
tau10<-1000
#B2
mu20<-0
tau20<-1000
#B3
mu30<-0
tau30<-1000
#S2
alfa0<-.01
beta0<-.01
# STORAGE OF RESULTADOS
resultados1 <- matrix(0,n.iterations,5)
resultados2 <- matrix(0,n.iterations,5)
colnames(resultados1)<-c("b0", "b1", "b2","b3", "var")
colnames(resultados2)<-c("b0", "b1", "b2","b3", "var")
# INITIAL VALUES ALREADY DEFINED
resultados1[1,] <- c(b0, b1, b2,b3, var)
resultados2[1,] <- c(b0_2, b1_2, b2_2,b3_2, var1)
#################### GIBBS SAMPLER WITH MH STEP FOR B1-INFORMATIVE PRIORS ####################
set.seed(6060714)
for(iteration in 2:n.iterations){
  mu01_1<-((sum(Y-b1*X1-b2*X2)/var) + (mu00/tau00)) / (N/var + 1/tau00)
  mu01_2<-((sum(Y-b1_2*X1-b2_2*X2)/var1) + (mu00/tau00)) / (N/var1 + 1/tau00)
  tau01_1<-1 / (N/var + 1/tau00) 
  tau01_2<-1 / (N/var1 + 1/tau00)
  
  b0   <-rnorm(1, mu01_1, sqrt(tau01_1))
  b0_2 <-rnorm(1, mu01_2, sqrt(tau01_2))
  
  betaStar<-rnorm(1,b1,1) ;betaStar
  ref<-runif(1,0,1) ;ref
  yStar<-sum(X1*(Y-b0-b2*X2)) ;yStar
  mnStar1<-(yStar/var+ mu10/tau10)/(sum(X1^2)/var+1/tau10)
  sgmStar1<-1/sqrt((sum(X1^2)/var+1/tau10))
  pc<-dnorm(b1,b1,1)
  pn<-dnorm(betaStar,b1,1)
  tc<-dnorm(b1,mnStar1,sgmStar1)
  tn<-dnorm(betaStar,mnStar1,sgmStar1)
  prob<-tn/tc*pc/pn
  if (ref<prob)(b1<-betaStar)
  
  betaStar2<-rnorm(1,b1_2,1) ;betaStar2
  ref2<-runif(1,0,1) ;ref2
  yStar2<-sum(X1*(Y-b0_2-b2_2*X2)) ;yStar2
  mnStar12<-(yStar2/var1+ mu10/tau10)/(sum(X1^2)/var1+1/tau10)
  sgmStar12<-1/sqrt((sum(X1^2)/var1+1/tau10))
  pc2<-dnorm(b1_2,b1_2,1)
  pn2<-dnorm(betaStar2,b1_2,1)
  tc2<-dnorm(b1_2,mnStar12,sgmStar12)
  tn2<-dnorm(betaStar2,mnStar12,sgmStar12)
  prob2<-tn2/tc2*pc2/pn2
  if (ref2<prob2)(b1_2<-betaStar2)
  
  mu21      <- ((sum(X2*(Y-b0-b1*X1))/var) + (mu20/tau20)) / (sum(X2^2)/var + 1/tau20)
  mu21_2    <- ((sum(X2*(Y-b0_2-b1_2*X1))/var1) + (mu20/tau20)) / (sum(X2^2)/var1 + 1/tau20)
  tau21     <- 1 / (sum(X2^2)/var + 1/tau20)
  tau21_2   <- 1 / (sum(X2^2/var1 + 1/tau20))
  
  b2         <- rnorm(1, mu21, sqrt(tau21))
  b2_2       <- rnorm(1,mu21_2,sqrt(tau21_2))
  
  mu31<-  ((sum(X3*(Y-b0-b1*X1-b2*X2))/var)+(mu30/tau30))/(sum(X3^2)/var +1/tau30)
  mu31_2<-((sum(X3*(Y-b0_2-b1_2*X1-b2_2*X2))/var1)+(mu30/tau30))/(sum(X3^2)/var1 +1/tau30)
  tau31<- 1/(sum(X3^2)/var + 1/tau30)
  tau31_2<- 1/(sum(X3^2)/var1 + 1/tau30)
  
  b3<-rnorm(1,mu31,sqrt(tau31))
  b3_2<-rnorm(1,mu31_2,sqrt(tau31_2))
  
  alfa1  <- N/2 + alfa0                                   ########################
  beta1  <-   sum((Y-(b0+b1*X1+b2*X2+b3*X3))^2)/2 + beta0 ########################
  beta1_2  <- sum((Y-(b0_2+b1_2*X1+b2_2*X2+b3_2*X3))^2)/2 + beta0 ################
  
  var <- 1/rgamma(1, alfa1, beta1)
  var1 <- 1/rgamma(1, alfa1, beta1_2)
  
  results1[iteration,]<- c(b0, b1, b2,b3, var)
  results2[iteration,]<-c(b0_2,b1_2,b2_2,b3_2,var1)
}

results.1<-as.data.frame(results1)
results.1$g<-1
results.1$n<-1:10000
results.2<-as.data.frame(results2)
results.2$g<-2
results.2$n<-1:10000
chains<-bind_rows(results.1,results.2)
#################### GIBBS SAMPLER WITH MH STEP FOR B1 - UNINFORMATIVE PRIORS ####################
set.seed(18573)
for(iteration in 2:n.iterations){
  mu01_1<-((sum(Y-b1*X1-b2*X2)/var) + (mu00/tau00)) / (N/var + 1/tau00)
  mu01_2<-((sum(Y-b1_2*X1-b2_2*X2)/var1) + (mu00/tau00)) / (N/var1 + 1/tau00)
  tau01_1<-1 / (N/var + 1/tau00) #sqrt???
  tau01_2<-1 / (N/var1 + 1/tau00)
  
  b0   <-rnorm(1, mu01_1, sqrt(tau01_1))
  b0_2 <-rnorm(1, mu01_2, sqrt(tau01_2))
  
  betaStar<-rnorm(1,b1,1) ;betaStar
  ref<-runif(1,0,1) ;ref
  yStar<-sum(X1*(Y-b0-b2*X2)) ;yStar
  mnStar1<-(yStar/var+ mu10/tau10)/(sum(X1^2)/var+1/tau10)
  sgmStar1<-1/sqrt((sum(X1^2)/var+1/tau10))
  pc<-dnorm(b1,b1,1)
  pn<-dnorm(betaStar,b1,1)
  tc<-dnorm(b1,mnStar1,sgmStar1)
  tn<-dnorm(betaStar,mnStar1,sgmStar1)
  prob<-tn/tc*pc/pn
  if (ref<prob)(b1<-betaStar)
  
  betaStar2<-rnorm(1,b1_2,1) ;betaStar2
  ref2<-runif(1,0,1) ;ref2
  yStar2<-sum(X1*(Y-b0_2-b2_2*X2)) ;yStar2
  mnStar12<-(yStar2/var1+ mu10/tau10)/(sum(X1^2)/var1+1/tau10)
  sgmStar12<-1/sqrt((sum(X1^2)/var1+1/tau10))
  pc2<-dnorm(b1_2,b1_2,1)
  pn2<-dnorm(betaStar2,b1_2,1)
  tc2<-dnorm(b1_2,mnStar12,sgmStar12)
  tn2<-dnorm(betaStar2,mnStar12,sgmStar12)
  prob2<-tn2/tc2*pc2/pn2
  if (ref2<prob2)(b1_2<-betaStar2)
  
  mu21      <- ((sum(X2*(Y-b0-b1*X1))/var) + (mu20/tau20)) / (sum(X2^2)/var + 1/tau20)
  mu21_2    <- ((sum(X2*(Y-b0_2-b1_2*X1))/var1) + (mu20/tau20)) / (sum(X2^2)/var1 + 1/tau20)
  tau21     <- 1 / (sum(X2^2)/var + 1/tau20)
  tau21_2   <- 1 / (sum(X2^2/var1 + 1/tau20))
  
  b2         <- rnorm(1, mu21, sqrt(tau21))
  b2_2       <- rnorm(1,mu21_2,sqrt(tau21_2))
  
  mu31<-  ((sum(X3*(Y-b0-b1*X1-b2*X2))/var)+(mu30/tau30))/(sum(X3^2)/var +1/tau30)
  mu31_2<-((sum(X3*(Y-b0_2-b1_2*X1-b2_2*X2))/var1)+(mu30/tau30))/(sum(X3^2)/var1 +1/tau30)
  tau31<- 1/(sum(X3^2)/var + 1/tau30)
  tau31_2<- 1/(sum(X3^2)/var1 + 1/tau30)
  
  b3<-rnorm(1,mu31,sqrt(tau31))
  b3_2<-rnorm(1,mu31_2,sqrt(tau31_2))
  
  alfa1  <- N/2 + alfa0                                   ########################
  beta1  <-   sum((Y-(b0+b1*X1+b2*X2+b3*X3))^2)/2 + beta0 ########################
  beta1_2  <- sum((Y-(b0_2+b1_2*X1+b2_2*X2+b3_2*X3))^2)/2 + beta0 ################
  
  var <- 1/rgamma(1, alfa1, beta1)
  var1 <- 1/rgamma(1, alfa1, beta1_2)
  
  resultados1[iteration,]<- c(b0, b1, b2,b3, var)
  resultados2[iteration,]<-c(b0_2,b1_2,b2_2,b3_2,var1)
}

resultados.1<-as.data.frame(resultados1)
resultados.1$g<-1
resultados.1$n<-1:10000
resultados.2<-as.data.frame(resultados2)
resultados.2$g<-2
resultados.2$n<-1:10000
chains_uni<-bind_rows(resultados.1,resultados.2)

#################### BURN-IN PERIOD: TRACE PLOT- INFORMATIVE PRIORS ####################
chains<-chains[-c(10001:14000),] 
chains<-chains[-c(1:4000),]

#################### BURN-IN PERIOD: TRACE PLOT - UNINFORMATIVE PRIORS####################
chains_uni<-chains_uni[-c(10001:14000),] 
chains_uni<-chains_uni[-c(1:4000),]

```

To evaluate convergence of this iterations, we first check at trace plots considering the total number of iterations and deleted the firs ones on where we can see that the chains were far to overlap. 4000 first iterations were deleted for each chain on each model, resulting in the following plots where convergence can be observed as a first check:

### Trace plots for $B_0$

```{r,echo=F, message=F, warning=F, fig.show="hold", out.width="50%"}
par(mfrow=c(1,2))
ggplot(chains,aes(x=n,y=b0,group=g,color=factor(g)))+ggtitle("Model 1")+
  geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
ggplot(chains_uni,aes(x=n,y=b0,group=g,color=factor(g)))+ggtitle("Model 2")+
    geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
```

### Trace plots for $B_1$ 

```{r,echo=F, message=F, warning=F, fig.show="hold", out.width="50%"}
par(mfrow=c(1,2))
ggplot(chains,aes(x=n,y=b1,group=g,color=factor(g)))+ggtitle("Model 1")+
  geom_line(size=.5,alpha=.7)+geom_point(size=.5)+theme(legend.position = "none")
ggplot(chains_uni,aes(x=n,y=b1,group=g,color=factor(g)))+ggtitle("Model 2")+
    geom_line(size=.5,alpha=.7)+geom_point(size=.5)+theme(legend.position = "none")
```

### Trace plots for $B_2$ 

```{r,echo=F, message=F, warning=F, fig.show="hold", out.width="50%"}
par(mfrow=c(1,2))
ggplot(chains,aes(x=n,y=b2,group=g,color=factor(g)))+ggtitle("Model 1")+
  geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
ggplot(chains_uni,aes(x=n,y=b2,group=g,color=factor(g)))+ggtitle("Model 2")+
    geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
```

### Trace plots for $B_3$ 

```{r,echo=F, message=F, warning=F, fig.show="hold", out.width="50%"}
par(mfrow=c(1,2))
ggplot(chains,aes(x=n,y=b3,group=g,color=factor(g)))+ggtitle("Model 1")+
  geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
ggplot(chains_uni,aes(x=n,y=b3,group=g,color=factor(g)))+ggtitle("Model 2")+
    geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
```

### Trace plots for $\sigma^{2}$ 

```{r,echo=F, message=F, warning=F, fig.show="hold", out.width="50%"}
par(mfrow=c(1,2))
ggplot(chains,aes(x=n,y=var,group=g,color=factor(g)))+ggtitle("Model 1")+
  geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
ggplot(chains_uni,aes(x=n,y=var,group=g,color=factor(g)))+ggtitle("Model 2")+
    geom_line(size=1.5,alpha=.1)+geom_point(size=.5)+theme(legend.position = "none")
```

We checked density plots as an approximation of the marginal posterior distribution, and in this case the ones for $B_0, B_2, B_3$ and variance look normal, for both models. Also, for both models, since $B_1$ was computed with Metropolis Hasting it was expected to look far away for normal.

Finally, we checked Monte Carlo Error and they all stay \<.05%. So, we can conclude that the models had converged, and we can continue with further analysis.

```{r,echo=F, message=F, warning=F,results='hide'}
#################### COVERGENCE: DENSITY PLOT - INFORMATIVE PRIORS ####################
b0_den<-ggplot(chains,aes(x=b0))+geom_density() 
b1_den<-ggplot(chains,aes(x=b1))+geom_density() 
b2_den<-ggplot(chains,aes(x=b2))+geom_density() 
b3_den<-ggplot(chains,aes(x=b3))+geom_density() 
var_den<-ggplot(chains,aes(x=var))+geom_density() 
#################### COVERGENCE: DENSITY PLOT - UNINFORMATIVE PRIORS ####################
b0_denu<-ggplot(chains_uni,aes(x=b0))+geom_density()
b1_denu<-ggplot(chains_uni,aes(x=b1))+geom_density()
b2_denu<-ggplot(chains_uni,aes(x=b2))+geom_density() 
b3_denu<-ggplot(chains_uni,aes(x=b3))+geom_density() 
var_denu<-ggplot(chains_uni,aes(x=var))+geom_density() 
#################### CONVERGENCE: MONTE CARLO ERROR - INFORMATIVE PRIORS ####################
#SD of estimates divided by the square root of the number of iterations
chains<-chains[-c(6:7)]
mce <- matrix(0,5,1)
mce[,1] <- apply(chains,2,sd)
mc_b0<-(.198324117/141.4214)
mc_b1<-(.030114044/141.4214)
mc_b2<-(.032625471/141.4214)
mc_b3<-(.009682962/141.4214)
mc_var<-(.139121898/141.4214)
mc<-rbind(mc_b0,mc_b1,mc_b2,mc_b3,mc_var) 
mce<-cbind(mce,mc)
colnames(mce) <- c("Sd","Monte Carlo error")
rownames(mce) <- c(c("b0", "b1", "b2","b3", "var")) 
mce
#################### CONVERGENCE: MONTE CARLO ERROR - UNINFORMATIVE PRIORS####################
#SD of estimates divided by the square root of the number of iterations
chains_uni<-chains_uni[-c(6:7)]
mce_uni <- matrix(0,5,1)
mce_uni [,1] <- apply(chains_uni,2,sd)
mc_b0_uni <-(.197028445/141.4214)
mc_b1_uni <-(.035867065/141.4214)
mc_b2_uni <-(.033322104/141.4214)
mc_b3_uni <-(.009709672/141.4214)
mc_var_uni <-(.137593451/141.4214)
mc_uni <-rbind(mc_b0_uni,mc_b1_uni,mc_b2_uni,mc_b3_uni,mc_var_uni ) 
mce_uni <-cbind(mce_uni ,mc_uni )
colnames(mce_uni ) <- c("Sd","MC error")
rownames(mce_uni ) <- c(c("b0", "b1", "b2","b3", "var")) 
  mce_uni
```


# Posterior predictive check

To check the assumption of normality of residuals we first stablish the null hypotheses as our proposed linear regression model: $$y_i=B_0+B_1*X1_i+B_2*X2_i+B_3*X3_i+e_i$$ with $$e \sim \mathcal{N}(\mu,\,\sigma^{2})$$

Second, we sample the posterior distribution of the model parameter, this is for each sampled parameters $\theta$ = [$B_0, B_1, B_2, B_3, \sigma^{2}$] computed in every 12000 iterations of our Gibbs sampler, and then we modelled $Y$ as a regression with fixed effect of $X1, X2$ and $X3$. So, we end up a matrix of 12000 columns and 1524 rows, where every box reflects and simulated outcome per subject of the observed data using a different set of estimates for every column.

Now we compute the residuals for the observed data: This is, we subtract to the observed Y of every subject the estimate predicted with the 12000 sets of $\theta$. To compute the residuals for the simulated data, we subtract the same estimated with the 12000 $\theta$ to the modelled Y that we previously obtain by simulating the outcomes maintaining $X1, X2$, and $X3$ fixed. Then, we compute the skewness in the absolute value = \|3 \* (Mean -- Median) / Standard Deviation\|, to avoid problems after when calculating the bayesian p-value like for example having a skewness coefficient of -.5 being considered as less skew than a coefficient of 1. Finally, we count how many times the skewness on the simulated data is larger than the skewness in the observed data, and we storage the results in a matrix of 12000 columns with 1 row where every time we will put a 0 if the coefficient of the observed data is larger than the simulated, and a 1 if it is the other way around. This procedure is computed in the exact same way for both models.

For model 1, there were 11526 occasions where skewness was larger for simulated data, leading to a bayesian p-value of 0.96. For model two, there were 7714 occasions where skewness was larger for simulated data, leading to a p-value of 0.64. This shows that for both models, assumption of normality of residuals is met.

```{r,echo=F, message=F, warning=F,results='hide'}
#################### POSTERIOR PREDICTIVE P VALUE FOR NORMALITY ASSUMPTION -INFORMATIVE PRIORS ####################
Y.est<-matrix(0,1524,12000) #Posterior predictive distribution 
for(i in 1:12000){
  Yi<-chains[i,1]+chains[i,2]*X1+chains[i,3]*X2+chains[i,4]*X3
  Y.est[,i]<-Yi}
#Compute residuals for observed data
res.obs<-matrix(0,1524,12000)
for(i in 1:12000){
  r.obs<-Y-chains[i,1]-chains[i,2]*X1-chains[i,3]*X2-chains[i,4]*X3
  res.obs[,i]<-r.obs}
#Compute residuals for simulated data
res.sim<-matrix(0,1524,12000)
for(i in 1:12000){
  r.sim<-Y.est[i]-chains[i,1]-chains[i,2]*X1-chains[i,3]*X2-chains[i,4]*X3
  res.sim[,i]<-r.sim}
#Skewness of the observed data
skw.obs<-matrix(0,1,12000)
for (i in 1:12000){
  s.obs<-abs(3*(mean(res.obs[,i])-median(res.obs[,i])/sd(res.obs[,i])))
  skw.obs[,i]<-s.obs}
#Skewness of the simulated data
skw.sim<-matrix(0,1,12000)
for (i in 1:12000){
  s.sim<-abs(3*(mean(res.sim[,i])-median(res.sim[,i])/sd(res.sim[,i])))
  skw.sim[,i]<-s.sim}
#Compute how many times skewness is larger in simulated data than in observed data
test.stat<-matrix(0,1,12000)
for(i in 1:12000){
  if(skw.sim[i]>skw.obs[i])
  {    (test.stat[i]<-1);  }
  else{    (test.stat[i]<-0); }}

table(test.stat) # 0= 474 1=11526 
mean(test.stat)  # 0.9605
#################### POSTERIOR PREDICTIVE P VALUE FOR NORMALITY ASSUMPTION - UNINFORMATIVE PRIORS ####################
Y.est.unin<-matrix(0,1524,12000) #Posterior predictive distribution 
for(i in 1:12000){
  Yi.uni<-chains_uni[i,1]+chains_uni[i,2]*X1+chains_uni[i,3]*X2+chains_uni[i,4]*X3
  Y.est.unin[,i]<-Yi.uni}
#Compute residuals for observed data
res.obs.uni<-matrix(0,1524,12000)
for(i in 1:12000){
  r.obs.uni<-Y-chains_uni[i,1]-chains_uni[i,2]*X1-chains_uni[i,3]*X2-chains_uni[i,4]*X3
  res.obs.uni[,i]<-r.obs.uni}
#Compute residuals for simulated data
res.sim.uni<-matrix(0,1524,12000)
for(i in 1:12000){
  r.sim.uni<-Y.est.unin[i]-chains_uni[i,1]-chains_uni[i,2]*X1-chains_uni[i,3]*X2-chains_uni[i,4]*X3
  res.sim.uni[,i]<-r.sim.uni}
#Skweness of the observed data
skw.obs.uni<-matrix(0,1,12000)
for (i in 1:12000){
  s.obs.uni<-abs(3*(mean(res.obs.uni[,i])-median(res.obs.uni[,i])/sd(res.obs.uni[,i])))
  skw.obs.uni[,i]<-s.obs.uni}
#Skweness of the simulated data
skw.sim.uni<-matrix(0,1,12000)
for (i in 1:12000){
  s.sim.uni<-abs(3*(mean(res.sim.uni[,i])-median(res.sim.uni[,i])/sd(res.sim.uni[,i])))
  skw.sim.uni[,i]<-s.sim.uni}
#Compute how many times skweness is larger in simulated data than in observed data
test.stat.uni<-matrix(0,1,12000)
for(i in 1:12000){
  if(skw.sim.uni[i]>skw.obs.uni[i])
  { (test.stat.uni[i]<-1);  }
  else{    (test.stat.uni[i]<-0); }}

table(test.stat.uni) # 0= 430 1=11570 
mean(test.stat.uni)  # 0.9641667
```

# Results and interpretation


```{r,echo=F, message=F, warning=F}
output_inf<-matrix(0,5,4)
rownames(output_inf)<-c("b0", "b1", "b2","b3", "var")
colnames(output_inf)<-c("Mean","SD","Q025","Q975")
output_inf[,1]<-apply(chains,2,mean)
output_inf[,2]<-apply(chains,2,sd)
output_inf[,3]<-apply(chains,2,quantile,0.025)
output_inf[,4]<-apply(chains,2,quantile,0.975)
kable(output_inf, caption = "Results for informative priors")
```


```{r,echo=F, message=F, warning=F}
output_unin<-matrix(0,5,4)
rownames(output_unin)<-c("b0", "b1", "b2","b3", "var")
colnames(output_unin)<-c("Mean","SD","Q025","Q975")
output_unin[,1]<-apply(chains_uni,2,mean)
output_unin[,2]<-apply(chains_uni,2,sd)
output_unin[,3]<-apply(chains_uni,2,quantile,0.025)
output_unin[,4]<-apply(chains_uni,2,quantile,0.975)
kable(output_unin, caption = "Results for uninformative priors")
```

As we can see in both tables, on average all estimates for the intercept of $B_0$ are positive,

For $B_0$ average results are positive, with a mean of 5.764 and CI [5.3-6.1] in first model and mean of 5.683 and CI [5.3-6-1] in second model. For $B_1$ average results are positive, with a mean of 0.065 and CI [0.0-0.11] in first model and mean of 0.1 and CI [0.03-0.16] in second model. For $B_2$ average results are negative, with a mean of -0.15 and CI [-0.21 - -0.09] in first model and mean of -0.177 and CI [-0.24- -0.113] in second model. For $B_3$ average results are positive, with a mean of 0.0 and CI [-0.01- 0.02] in first model and mean of 0.002 and CI [-0.01-0.02] in second model.

For both models, coefficients $B_0, B_2$ and $B_3$ could be interpret as evidence that they are different from 0 and have a positive association with the outcome, since their credible intervals do not include 0. In the case of $B_3$ there is not enough clear evidence that the coefficient is different from 0 and its direction its not clear since their credible intervals include 0.

# Model selection using the DIC

To compare these models, we refer to the Deviance Information Criterion.

AIC for model with informative priors is 6365.9, whereas for model with uninformative priors is 6366.3. This difference of 0.4 does not allows to discriminate for AIC which one is the better model. Since both might be equivalent, we will stay with the one with prior information for next analysis.

```{r,echo=F, message=F, warning=F,results='hide'}
#################### COMPARING MODELS: DIC ####################
dic.inf<-matrix(0,12000,1)
for(i in 1:12000){
  mean<-(chains$b0[i]+chains$b1[i]*X1+chains$b2[i]*X2+chains$b3[i]*X3)
  sd<-sqrt(chains$var[i])
  dic.inf[i]<-sum(dnorm(Y,mean,sd,log=T))
}
mean(dic.inf)*-2 #6365.902

dic.uninf<-matrix(0,12000,1)
for(i in 1:12000){
  mean<-(chains_uni$b0[i]+chains_uni$b1[i]*X1+chains_uni$b2[i]*X2+chains_uni$b3[i]*X3)
  sd<-sqrt(chains_uni$var[i])
  dic.uninf[i]<-sum(dnorm(Y,mean,sd,log=T))
    }
mean(dic.uninf)*-2 #6366.347

6366.849-6366.347 #0.502
```

# Model selection using bayes factor

After concluding that model that uses prior information was the better model when selecting using DIC, 10 different hypotheses were tested concerning each of the predictor being larger or smaller than 0:

```{r,echo=F, message=F, warning=F,results='hide'}
m1<-lm(scale~prl+lgl+plc,data = df)
set.seed(43838)
#set.seed(78874) #This seed was also run to check stability
bf<-bain(m1," prl=0 & lgl=0 & plc=0;
              prl>0 & lgl>0 & plc>0; 
              prl>0 & lgl>0 & plc<0;
              prl>0 & lgl<0 & plc>0;
              prl>0 & lgl<0 & plc<0;
              prl<0 & lgl<0 & plc<0;
              prl<0 & lgl<0 & plc>0;
              prl<0 & lgl>0 & plc>0;
              prl>0 & lgl<0 & plc<0;
              prl<0 & lgl>0 & plc<0",fraction=1,standardize=F)
```


```{r table, echo=FALSE, message=FALSE, warnings=FALSE}

df1 <- tibble(
  Hypotheses = c("H0 = B1 = 0, B2 = 0, B3 = 0", 
                 "H1 = B1 > 0, B2 > 0, B3 > 0", 
                 "H2 = B1 > 0, B2 > 0, B3 < 0", 
                 "H3 = B1 > 0, B2 < 0, B3 > 0",
                 "H4 = B1 > 0, B2 < 0, B3 < 0",
                 "H5 = B1 < 0, B2 < 0, B3 < 0",
                 "H6 = B1 < 0, B2 < 0, B3 > 0",
                 "H7 = B1 < 0, B2 > 0, B3 > 0",
                 "H8 = B1 > 0, B2 < 0, B3 < 0",
                 "H9 = B1 < 0, B2 > 0, B3 < 0"),
  BF.u = c("0.005","0.000","0.000","5.457","0.070","0.002","0.243","0.000","0.066","0.000"),
  BF.c = c("0.005","0.000","0.000","103.47","0.060","0.002","0.218","0.000","0.056","0.000"),
  PMPa = c("0.001","0.000","0.000","0.934","0.012","0.000","0.042","0.000","0.011","0.000"),
  PMPb = c("0.001","0.000","0.000","0.797","0.010","0.000","0.035","0.000","0.010","0.000")
)    
kable(df1, caption = "Testing of hypotheses with bayesian factor")
```


These hypotheses were tested using the package Bain in R, trying different seets to ensure the stability of the results. From it we can observed that according to the column BF.u that indicates the Bayes factor as a ratio of fit and complexity, the most strong hypotheses to be supported among the one tested is the third one.

That indicates that to predict out outcome variable $Y$- Political Self-positioning, the coefficient $B_1$- Trust on country's parliament would have a positive association with $Y$, Trust in legal system would have a negative one, and Trust in Police would have a positive one. This is, the more a subject trust in parliament and police, would be place more to the right in the scale of political self-positioning. And works in the opposite way for trust in the legal system, for each point of trust on it that a person has, it would be placed more to the left.


# Bayesian and frequentist approach

```{r,echo=F, message=F, warning=F,results='hide'}
#################### FREQUENTIST APPROACH ####################
Yfreq<-lm(scale~prl+lgl+plc,data=df)
summary(Yfreq)
```

Finally, we estimate a linear regression in the frequentist approach, and we can see that the results barely differ from the ones obtained with a bayesian approach. 
Coefficients for B0= 5.41177, B1= 0.06598, B2= -0.19384, B3= 0.08784 and Residual standard error= 1.95. All coefficients significant to p<.05.
Because of this is difficult to observe any benefit that could have to adopt this approach, since we also could observe that using informative and uninformative priors did not make any difference. 

# Source

ESS Round 8: European Social Survey Round 8 Data (2016). Data file edition 2.2. NSD - Norwegian Centre for Research Data, Norway ‚Äì Data Archive and distributor of ESS data for ESS ERIC. doi:10.21338/NSD-ESS8-2016.
